<!DOCTYPE HTML>
<html lang="en">

<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yuqun Wu</title>
  
  <meta name="author" content="Yuqun Wu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="tVUcHkaWPb9nJyOaZTBWTKERRFr31sk97vIEOwmXvNo" />
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
<!--	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">-->
  <link rel="icon" href="images/illinois_icon.png" type="image/x-icon">

</head>



<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yuqun Wu</name>
              </p>
              <p>
                I am a third year Ph.D. student in Computer Science at the University of Illinois at Urbana-Champaign, advised by <a href="http://dhoiem.cs.illinois.edu">Prof. Derek Hoiem</a>. I also work closely with <a href="https://shenlong.web.illinois.edu">Prof. Shenlong Wang</a>. 
              </p>
              <p>
                I received my MS and BS degrees from UIUC, with my undergraduate degree in Computer Science & Statistics. I spent the summer of 2025 at Meta and did a summer intern at UCSD in 2022. 
              </p>
              <p style="text-align:center">
                <a href="mailto:yuqunwu2@illinois.edu">Email</a> &nbsp/&nbsp
                <a href="data/Yuqun_Resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/yuqun-wu-98b8761a2/">Linkedin</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/yuqun.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yuqun.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My research interests mainly lie in computer vision, especially for 3D scene understanding, video understanding, neural rendering, and geometry reconstruction.</li>
            </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SceneDiff.jpg" alt="clean-usnob" width="250" height="100">
            </td>
            <td width="75%" valign="middle">
              <papertitle>SceneDiff: A Benchmark and Method for Multiview Object Change Detection</papertitle>
              <br>
              <strong>Yuqun Wu</strong>,
              <a href="https://chih-hao-lin.github.io">Chih-hao Lin</a>,
              <a href="https://hungdche.github.io/">Henry Che</a>,
              <a href="https://adititiwari19.github.io">Aditi Tiwari</a>,
              <a href="https://zouchuhang.github.io">Chuhang Zou</a>,
              <a href="https://shenlong.web.illinois.edu">Shenlong Wang</a>,
              <a href="http://dhoiem.cs.illinois.edu">Derek Hoiem</a>
              <br>
              <em>Under Review</em>
              <br>
              <a href="https://yuqunw.github.io/SceneDiff/">Project Page</a>
              , <a href="https://arxiv.org/abs/2512.16908">Preprint</a>
              , <a href="https://github.com/yuqunw/scene_diff">Code</a>
              , <a href="https://huggingface.co/datasets/yuqun/SceneDiff">Dataset</a>
              , <a href="https://github.com/yuqunw/scenediff_annotator">Annotator</a>
              <p> Propose a dataset and a new method for object change detection between a pair of captures (images or videos) of the same scene at different times. </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/text_region.png" alt="clean-usnob" width="250" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models</papertitle>
              <br>
              <a href="https://avaxiao.github.io">Yao Xiao</a>,
              <a href="https://github.com/QiqianFu">Qiqian Fu</a>,
              <a href="https://www.linkedin.com/in/heyi-tao-916670174/">Heyi Tao</a>,
              <strong>Yuqun Wu</strong>,
              <a href="https://zzhu.vision">Zhen Zhu</a>,
              and <a href="http://dhoiem.cs.illinois.edu">Derek Hoiem</a>
              <br>
              <em>TMLR 2025</em>
              <br>
              <a href="https://arxiv.org/pdf/2505.23769">Preprint</a>
              , <a href="https://github.com/avaxiao/TextRegion?tab=readme-ov-file">Code</a>
              <p> Propose a training-free approach to create text compatible region tokens, enabling powerful zero-shot region-level understanding with existing image-text models. </p>
            </td>
          </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MonoPatchNeRF.png" alt="clean-usnob" width="250" height="150">
            </td>
            <td width="75%" valign="middle">
              <papertitle>MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based Monocular Guidance</papertitle>
              <br>
              <strong>Yuqun Wu*</strong>,
              <a href="https://jyl.kr">Jae Yong Lee*</a>,
              <a href="https://zouchuhang.github.io">Chuhang Zou</a>,
              <a href="https://shenlong.web.illinois.edu">Shenlong Wang</a>
              , and <a href="http://dhoiem.cs.illinois.edu">Derek Hoiem</a>
              <br>
              <em>3DV 2025</em>
              <br>
              <a href="https://yuqunw.github.io/MonoPatchNeRF/">Project Page</a>
              , <a href="https://arxiv.org/abs/2404.08252">Preprint</a>
              , <a href="https://github.com/yuqunw/monopatch_nerf?tab=readme-ov-file">Code</a>
              <p> Create 3D models that provide accurate geometry and view synthesis, partially closing the large geometric performance gap between NeRF and traditional MVS methods</p>
            </td>
          </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ppng.png" alt="clean-usnob" width="250" height="150">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Plenoptic PNG: Real-Time Neural Radiance Fields in 150 KB</papertitle>
              <br>
              <a href="https://jyl.kr">Jae Yong Lee</a>,
              <strong>Yuqun Wu</strong>,
              <a href="https://zouchuhang.github.io">Chuhang Zou</a>,
              <a href="http://dhoiem.cs.illinois.edu">Derek Hoiem</a>
              , and <a href="https://shenlong.web.illinois.edu">Shenlong Wang</a>
              <br>
              <em>3DV 2025</em>
              <br>
              <a href="https://jyl.kr/ppng/">Project Page</a>
              , <a href="https://arxiv.org/abs/2409.15689">Preprint</a>
              , <a href="https://github.com/leejaeyong7/ppng">Code</a>
              <p> Encode 3D scenes into extremely compact representation from 2D images and enable its transmittance, decoding and rendering in real-time across various platforms via traditional GL pipeline.</p>
            </td>
          </tr>

            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dino_sam.png" alt="clean-usnob" width="250" height="100">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Region-Based Representations Revisited</papertitle>
              <br>
              <a href="https://michalmsr.web.illinois.edu">Michal Shlapentokh-Rothman*</a>,
              Ansel Blume*,
              <a href="https://avaxiao.github.io">Yao Xiao</a>,
              <strong>Yuqun Wu</strong>,
              <a href="https://sethuramantv001.wixsite.com/sethu">Sethuraman T V</a>,
              <a href="https://www.linkedin.com/in/heyi-tao-916670174/">Heyi Tao</a>,
              <a href="https://jyl.kr">Jae Yong Lee</a>,
              <a href="https://www.linkedin.com/in/wilfredo-torres-calder√≥n-0468b532/?locale=en_US">Wilfredo Torres</a>,
              <a href="https://yxw.web.illinois.edu">Yu-Xiong Wang</a>,
              and <a href="http://dhoiem.cs.illinois.edu">Derek Hoiem</a>
              <br>
              <em>CVPR 2024</em>
              <br>
              <a href="https://regionreps.web.illinois.edu">Project Page</a>
              , <a href="https://arxiv.org/abs/2402.02352">Preprint</a>
              , <a href="https://github.com/michalsr/regions">Code</a>
              <p>Investigate region based representation, combining class-agnostic segmentation from SAM and dense features from foundation models, for a wide variety of tasks, including semantic segmentation, object-based image retrieval, and multi-image analysis.</p>
            </td>
          </tr>


            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SSPN.png" alt="clean-usnob" width="250" height="140">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Sparse SPN: Depth Completion from Sparse Keypoints</papertitle>
              <br>
              <strong>Yuqun Wu*</strong>, <a href="https://jyl.kr">Jae Yong Lee*</a>, and <a href="http://dhoiem.cs.illinois.edu">Derek Hoiem</a>
              <br>
              <a href="https://arxiv.org/abs/2212.00987">Preprint</a>
              <p>Propose a novel method that outperforms existing depth completion pipelines given sparse keypoint depth, and reconstructs complete point clouds given SfM setups</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/QFF.png" alt="clean-usnob" width="250" height="70">
            </td>
            <td width="75%" valign="middle">
              <papertitle>QFF: Quantized Fourier Features for Neural Field Representations</papertitle>
              <br>
              <a href="https://jyl.kr">Jae Yong Lee</a>, <strong>Yuqun Wu</strong>, <a href="https://zouchuhang.github.io">Chuhang Zou</a>, <a href="https://shenlong.web.illinois.edu">Shenlong Wang</a>, and <a href="http://dhoiem.cs.illinois.edu">Derek Hoiem</a>
              <br>
              <a href="https://arxiv.org/abs/2212.00914">Preprint</a>
              <p>Present Quantized Fourier Features (QFF), which encodes features in bins of Fourier features, and can result in smaller model size, faster training, and better quality outputs for various applications of neural representation</p>
            </td>
          </tr>

        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Service</heading>
              <br>
              <p><strong>Teaching Assistant</strong>: <a href="https://courses.engr.illinois.edu/cs445/fa2022/">Computational Photography (Fall 2022)</a>, <a href="https://courses.engr.illinois.edu/cs441/sp2023/">Applied Machine Learning (Spring 2023)</a></p>
              <p><strong>Reviewer</strong>: CVPR 2025, ECCV 2024, WACV 2025</p>
            </td>
            </tr>
        </tbody></table>
      </td>
    </tr>

  <tr>
            <td style="padding:0px">
              <p style="text-align:right;font-size:small;">

              Updated on Sep 5 2025. Thanks <a href="https://jonbarron.info">Jon Barron</a> for his amazing template.
              </p>
            </td>
  </tr>
  </tbody>
  </table>
</body>

</html>
